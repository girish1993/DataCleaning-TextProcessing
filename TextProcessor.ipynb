{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Text2XML\n",
    "####  Name: Girish Bhatta\n",
    "\n",
    "Date: 31/08/2018\n",
    "\n",
    "Environment: Python 3.6 and Jupyter notebook\n",
    "Libraries used: \n",
    "* nltk - natural language toolkit (tokenizer, stopwords, collocations and probabilities)\n",
    "* re (for regular expression, included in Anaconda Python 3.6) \n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This analysis extracts data from an text based corpus which has 250 resume documents. Data was extracted by reading the allotted input files for each student in resume_student.txt files. The names of the allotted files were maintained in a list and were read one after the other by iterating the list items and then capturing the file data in a dictionary where the key is the filename and the value is the data of that file.\n",
    "\n",
    "The list of alloted files had duplicate entries. These duplicate entries were ommited while reading the files. \n",
    "\n",
    "Text pre-processing was performed with the objective of producing a lexical vocabulary for the resumes and the associated sparse count vector for each abstract. The pre-processing included tokenisation, stemming and removal of stopwords. Additionally, the most frequent and least frequent words were removed, and meaningful bigrams were identified.  The initial tokenised vocabulary of the corpus was 9690 words, which was reduced to 2334 words following pre-processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing natural language toolkit\n",
    "import nltk\n",
    "\n",
    "#importing ngrms from nltk.util\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#importing regexptokenizer from mltk.tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#importing all functions from nltk.probability to calculate frequency distribution and other functions\n",
    "from nltk.probability import *\n",
    "import nltk.data\n",
    "\n",
    "#importing Regular expression library\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading the Allotted Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder \"resumeTxt\" contains 866 files, where each file represents a resume. Each resume is unstructured text file. In order to process the data, the data of each file is read and stored as a list of dictionaries where each dictionary represents a resume document.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inital loading of all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files read :218\n"
     ]
    }
   ],
   "source": [
    "#list of all the files that needs to be read\n",
    "files = [\"446 857 219 751 750 784 548  84 234 775 807 355 106 119 198 243 519 813 476 155 341 274 256 308 401 743 640 312  29 626 474 178 467 789 358 593 818 236 661 352 571 382 860  35  84 411 772 144 336 525 705 341 537 656 294 286 247 773 807 224 394 612 480 844 586 170 583 784 616 662 349 366 644 799 828 556   6 643 458 492 660 757 572 606 659 810 311 293 729 472 845   4 529 401 352 291 208 519 686 687 844 682 707 539  72 164 619 496 168  90 299 661 635 670 125 765 623 403 540  58 665 284 801 342 767 810 44 434  78 393 838 731 309 167 265 778 609 769 681 771 220 301 829 131 358 759 519 362  13 536 349 576  95 466 665 413 858 659 839 540 732 132 773   3 324 346 849 604 307 548 564 479 351 279 728 270 539 549 590 771 13 397 629 300 525 775  72 597 608 288 278 842  62 283 677 381 265 417 736 827 667 216  99 107 225 820 554 811 347 161 535 469 383 111 206 279 569 644 136 550  10 139 629 139 625 499 463 369 607 379 557 372 824 638 505 289 794 619 356  19 240  77   3  59 457 294 509 238 579 531\"]\n",
    "\n",
    "# splitting by the space to get each file entry and then getting rid of emtpy entries in the list \n",
    "list_of_files = files[0].split(\" \")\n",
    "list_of_files = list(filter(None, list_of_files))\n",
    "\n",
    "\n",
    "        \n",
    "list_of_files = list(set(list_of_files))\n",
    "list_of_files.sort()\n",
    "\n",
    "#In order for the reading of files to be successful, It must be ensured that this .ipybnb file has been placed within the same\n",
    "#directory of text files. If not, then this would throw an error and further execution cannot be carried out. \n",
    "\n",
    "\n",
    "#list which will store dictionaries where each dictionary represents a resume document\n",
    "file_data_list = []\n",
    "\n",
    "count = 0\n",
    "for each_file in list_of_files:\n",
    "    count += 1\n",
    "    file_data = {}\n",
    "    filename = \"resume_(\"+each_file+\").txt\"\n",
    "    fh = open(filename,\"r\",encoding=\"utf8\")\n",
    "    file_data[each_file] = fh.read()\n",
    "    file_data_list.append(file_data)\n",
    "    fh.close()\n",
    "\n",
    "print(\"Total number of files read :\"+str(count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inspection of the structure of Data in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'10': 'Laurent Lapaire \\n\\nDate of birth: \\nNationality: \\nAddress: \\nMobile: \\nMail: \\n\\nEducation  \\n2017 \\n \\n2013 \\n\\n \\n2012 -13 \\n\\n \\n2010 \\n \\n\\n24 April 1990 \\nSwiss \\n26 Jalan Elok, Singapore 229064 \\n+65 8399 9433  \\nlapaire.laurent@gmail.com  \\n\\n \\n\\nChartered Accountant (Singapore) - ISCA \\n--end? Bachelor thesis, Geneva University) \\nBachelor in Business Administration, Geneva University, faculty of SES (Social and Economic \\nScience), Switzerland –Thesis on the future of the Chinese currency. (Lapaire, LJ 2013, Renminbi dead-\\nTwo semesters scholarship at Yonsei University, Seoul, South Korea to finalize my bachelor in Business \\nAdministration (international exchange program). \\n\\nHigh school degree at Collège Calvin, Geneva, Switzerland (specialization in law and economics) \\n\\nProfessiona l  Experience  \\n12/15 – present \\n\\n  Corporate Services Manager at Alpadis (Singapore) Pte. Ltd. In Singapore \\n-  Monthly preparation of consolidated financial reports, budget, cost reports for product lines \\n- \\nProvide recommendations to reduce costs and improve financial performance \\n- \\nEnsure all financial practices are in line with al statutory legislation and regulations \\n-  Confirm assets are being capitalized or expensed in accordance with the company accounting standard \\nand policy \\n\\n \\n\\n11/13 to 11/15 \\n\\n \\n08/13 to 11/13 \\n\\n06/13 to 08/13 \\n\\n \\nSummer 2011 \\n\\nAccountant at Swiss Fiduciary & Advisory Services Pte. Ltd. in Singapore. \\n- In charge for the accounts of 13 companies. \\n- Monthly bank reconciliations, GST return, preparation of Tax Return, XBRL \\n- Preparation of the financial statements on a quarterly basis. \\n- Introduced business process improvements that enhanced A/P functions, established common vendor   \\nfiles, eliminated duplications. \\n- Responsible for all company secretarial functions, duties and responsibilities  \\n\\nProject Manager at Intuuchina, Shanghai, China \\n- Responsible for cost estimates and planning for foreign companies relocating to China. \\n- Responsible for ensuring best value is obtained from suppliers and balancing the use of internal and \\nexternal resources. \\n \\n\\nConsulting Internship at Dongjin Consulting Co, Ltd, Shanghai, China \\n- Market research on specific topics \\n- Generating new business both in face to face meetings and over the phone. \\n\\nPrivate Banking Internship at SUNTRUST Investments SA, Geneva, Switzerland \\n- Market analysis upon personal request of private clients \\n- Preparing the daily morning news report and performing equity and bond analysis for the trading desk. \\n- Processing/validating and reconciliation of client information (contracts, orders, account cash \\nflows). \\n\\nLanguages  \\nFrench \\nEnglish \\nGerman \\nMandarin \\n\\nNative language \\nFluent \\nIntermediate level \\nBeginner \\n\\nHobbies \\nSkiing, Golfing, Gym, Managing an investment portfolio since 2011 \\n\\n\\x0c'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the content of the first file\n",
    "file_data_list[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks rather unstructured. But for initial preprocessing like sentence segmentation and Case normalization we have to get rid of the special characters that appear as part of the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Removal of special characters from the data of each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of special characters as part of preprocessing\n",
    "for each_res in file_data_list:\n",
    "    for k in each_res:    \n",
    "        all_spl = list(set(re.findall('^[^A-Za-z0-9\\,\\(\\)\\{\\}\\[\\]\\#\\@\\$\\%\\^\\&\\s\\+\\|\\.\\,\\:\\*]+',each_res[k],re.MULTILINE)))\n",
    "        for each_spl in all_spl:   \n",
    "            each_res[k] = each_res[k].replace(each_spl,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate through the list of dictionaries and replace each special occurence with an empty string. the regex <b>\"^[^A-Za-z0-9\\,\\(\\)\\{\\}\\[\\]\\#\\@\\$\\%\\^\\&\\s\\+\\|\\.\\,\\:\\*]+\"</b> can be dissected in the following way. all the special characters appeard to begin at the start of each new line hence the caret appears outside the character class. inside the character class we have another character which will look for anything, but the characters mentioned in the character class. all these are recorded in a findall of the regex and each of these special characters are the replaced with empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Parsing the Data for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Sentence detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'10': ['Laurent Lapaire \\n\\nDate of birth: \\nNationality: \\nAddress: \\nMobile: \\nMail: \\n\\nEducation  \\n2017 \\n \\n2013 \\n\\n \\n2012 13 \\n\\n \\n2010 \\n \\n\\n24 April 1990 \\nSwiss \\n26 Jalan Elok, Singapore 229064 \\n+65 8399 9433  \\nlapaire.laurent@gmail.com  \\n\\n \\n\\nChartered Accountant (Singapore)  ISCA \\nend?',\n",
       "   'Bachelor thesis, Geneva University) \\nBachelor in Business Administration, Geneva University, faculty of SES (Social and Economic \\nScience), Switzerland –Thesis on the future of the Chinese currency.',\n",
       "   '(Lapaire, LJ 2013, Renminbi dead\\nTwo semesters scholarship at Yonsei University, Seoul, South Korea to finalize my bachelor in Business \\nAdministration (international exchange program).',\n",
       "   'High school degree at Collège Calvin, Geneva, Switzerland (specialization in law and economics) \\n\\nProfessiona l  Experience  \\n12/15 – present \\n\\n  Corporate Services Manager at Alpadis (Singapore) Pte.',\n",
       "   'Ltd.',\n",
       "   'In Singapore \\n  Monthly preparation of consolidated financial reports, budget, cost reports for product lines \\n \\nProvide recommendations to reduce costs and improve financial performance \\n \\nEnsure all financial practices are in line with al statutory legislation and regulations \\n  Confirm assets are being capitalized or expensed in accordance with the company accounting standard \\nand policy \\n\\n \\n\\n11/13 to 11/15 \\n\\n \\n08/13 to 11/13 \\n\\n06/13 to 08/13 \\n\\n \\nSummer 2011 \\n\\nAccountant at Swiss Fiduciary & Advisory Services Pte.',\n",
       "   'Ltd. in Singapore.',\n",
       "   'In charge for the accounts of 13 companies.',\n",
       "   'Monthly bank reconciliations, GST return, preparation of Tax Return, XBRL \\n Preparation of the financial statements on a quarterly basis.',\n",
       "   'Introduced business process improvements that enhanced A/P functions, established common vendor   \\nfiles, eliminated duplications.',\n",
       "   'Responsible for all company secretarial functions, duties and responsibilities  \\n\\nProject Manager at Intuuchina, Shanghai, China \\n Responsible for cost estimates and planning for foreign companies relocating to China.',\n",
       "   'Responsible for ensuring best value is obtained from suppliers and balancing the use of internal and \\nexternal resources.',\n",
       "   'Consulting Internship at Dongjin Consulting Co, Ltd, Shanghai, China \\n Market research on specific topics \\n Generating new business both in face to face meetings and over the phone.',\n",
       "   'Private Banking Internship at SUNTRUST Investments SA, Geneva, Switzerland \\n Market analysis upon personal request of private clients \\n Preparing the daily morning news report and performing equity and bond analysis for the trading desk.',\n",
       "   'Processing/validating and reconciliation of client information (contracts, orders, account cash \\nflows).',\n",
       "   'Languages  \\nFrench \\nEnglish \\nGerman \\nMandarin \\n\\nNative language \\nFluent \\nIntermediate level \\nBeginner \\n\\nHobbies \\nSkiing, Golfing, Gym, Managing an investment portfolio since 2011']}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download the required packages to carry out sentence tokenization\n",
    "nltk.download('punkt')\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#sentence detection and making changes to the file_data_list\n",
    "for each_res in file_data_list:\n",
    "    for k in each_res:        \n",
    "        each_res[k] = sent_detector.tokenize(each_res[k].strip())\n",
    "        \n",
    "file_data_list[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of Data parsing, we first perform sentence tokenization. This is useful for case normalization.We can see as a result of sentence tokenization, the content of the file is <b>divided into sentences and returned as list of strings where each string represents a sentence.</b> we have to case normalize the first token/word of the file and concetentate them back together in order to be processed further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Case normalization and Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'10': 'laurent Lapaire \\n\\nDate of birth: \\nNationality: \\nAddress: \\nMobile: \\nMail: \\n\\nEducation  \\n2017 \\n \\n2013 \\n\\n \\n2012 13 \\n\\n \\n2010 \\n \\n\\n24 April 1990 \\nSwiss \\n26 Jalan Elok, Singapore 229064 \\n+65 8399 9433  \\nlapaire.laurent@gmail.com  \\n\\n \\n\\nChartered Accountant (Singapore)  ISCA \\nend?bachelor thesis, Geneva University) \\nbachelor in Business Administration, Geneva University, faculty of SES (Social and Economic \\nScience), Switzerland –Thesis on the future of the Chinese currency.(lapaire, LJ 2013, Renminbi dead\\nTwo semesters scholarship at Yonsei University, Seoul, South Korea to finalize my bachelor in Business \\nAdministration (international exchange program).high school degree at Collège Calvin, Geneva, Switzerland (specialization in law and economics) \\n\\nProfessiona l  Experience  \\n12/15 – present \\n\\n  Corporate Services Manager at Alpadis (Singapore) Pte.ltd.in Singapore \\n  Monthly preparation of consolidated financial reports, budget, cost reports for product lines \\n \\nProvide recommendations to reduce costs and improve financial performance \\n \\nEnsure all financial practices are in line with al statutory legislation and regulations \\n  Confirm assets are being capitalized or expensed in accordance with the company accounting standard \\nand policy \\n\\n \\n\\n11/13 to 11/15 \\n\\n \\n08/13 to 11/13 \\n\\n06/13 to 08/13 \\n\\n \\nSummer 2011 \\n\\nAccountant at Swiss Fiduciary & Advisory Services Pte.ltd. in Singapore.in charge for the accounts of 13 companies.monthly bank reconciliations, GST return, preparation of Tax Return, XBRL \\n Preparation of the financial statements on a quarterly basis.introduced business process improvements that enhanced A/P functions, established common vendor   \\nfiles, eliminated duplications.responsible for all company secretarial functions, duties and responsibilities  \\n\\nProject Manager at Intuuchina, Shanghai, China \\n responsible for cost estimates and planning for foreign companies relocating to China.responsible for ensuring best value is obtained from suppliers and balancing the use of internal and \\nexternal resources.consulting Internship at Dongjin consulting Co, Ltd, Shanghai, China \\n Market research on specific topics \\n Generating new business both in face to face meetings and over the phone.private Banking Internship at SUNTRUST Investments SA, Geneva, Switzerland \\n Market analysis upon personal request of private clients \\n Preparing the daily morning news report and performing equity and bond analysis for the trading desk.processing/validating and reconciliation of client information (contracts, orders, account cash \\nflows).languages  \\nFrench \\nEnglish \\nGerman \\nMandarin \\n\\nNative language \\nFluent \\nIntermediate level \\nBeginner \\n\\nHobbies \\nSkiing, Golfing, Gym, Managing an investment portfolio since 2011'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code snippet iterates through each file's data, which is currently list of strings, case normalises the first word of the sentence \n",
    "# and concatenates it back in the same format.\n",
    "for each_res in file_data_list:\n",
    "    for k in each_res:\n",
    "        sentence = \"\"\n",
    "        for each_sentence in each_res[k]:\n",
    "            sentence += each_sentence.replace(each_sentence[0:each_sentence.find(' ')],each_sentence[0:each_sentence.find(' ')].lower())\n",
    "        each_res[k] = sentence\n",
    "        \n",
    "file_data_list[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the result of case normalization in the above output. First word of each file has been case normalized and each of the sentences are put back together in the same format for tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Tokenization of file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'10': ['laurent',\n",
       "   'Lapaire',\n",
       "   'Date',\n",
       "   'of',\n",
       "   'birth',\n",
       "   'Nationality',\n",
       "   'Address',\n",
       "   'Mobile',\n",
       "   'Mail',\n",
       "   'Education',\n",
       "   '2017',\n",
       "   '2013',\n",
       "   '2012',\n",
       "   '13',\n",
       "   '2010',\n",
       "   '24',\n",
       "   'April',\n",
       "   '1990',\n",
       "   'Swiss',\n",
       "   '26',\n",
       "   'Jalan',\n",
       "   'Elok',\n",
       "   'Singapore',\n",
       "   '229064',\n",
       "   '65',\n",
       "   '8399',\n",
       "   '9433',\n",
       "   'lapaire',\n",
       "   'laurent',\n",
       "   'gmail',\n",
       "   'com',\n",
       "   'Chartered',\n",
       "   'Accountant',\n",
       "   'Singapore',\n",
       "   'ISCA',\n",
       "   'end',\n",
       "   'bachelor',\n",
       "   'thesis',\n",
       "   'Geneva',\n",
       "   'University',\n",
       "   'bachelor',\n",
       "   'in',\n",
       "   'Business',\n",
       "   'Administration',\n",
       "   'Geneva',\n",
       "   'University',\n",
       "   'faculty',\n",
       "   'of',\n",
       "   'SES',\n",
       "   'Social',\n",
       "   'and',\n",
       "   'Economic',\n",
       "   'Science',\n",
       "   'Switzerland',\n",
       "   'Thesis',\n",
       "   'on',\n",
       "   'the',\n",
       "   'future',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Chinese',\n",
       "   'currency',\n",
       "   'lapaire',\n",
       "   'LJ',\n",
       "   '2013',\n",
       "   'Renminbi',\n",
       "   'dead',\n",
       "   'Two',\n",
       "   'semesters',\n",
       "   'scholarship',\n",
       "   'at',\n",
       "   'Yonsei',\n",
       "   'University',\n",
       "   'Seoul',\n",
       "   'South',\n",
       "   'Korea',\n",
       "   'to',\n",
       "   'finalize',\n",
       "   'my',\n",
       "   'bachelor',\n",
       "   'in',\n",
       "   'Business',\n",
       "   'Administration',\n",
       "   'international',\n",
       "   'exchange',\n",
       "   'program',\n",
       "   'high',\n",
       "   'school',\n",
       "   'degree',\n",
       "   'at',\n",
       "   'Collège',\n",
       "   'Calvin',\n",
       "   'Geneva',\n",
       "   'Switzerland',\n",
       "   'specialization',\n",
       "   'in',\n",
       "   'law',\n",
       "   'and',\n",
       "   'economics',\n",
       "   'Professiona',\n",
       "   'l',\n",
       "   'Experience',\n",
       "   '12',\n",
       "   '15',\n",
       "   'present',\n",
       "   'Corporate',\n",
       "   'Services',\n",
       "   'Manager',\n",
       "   'at',\n",
       "   'Alpadis',\n",
       "   'Singapore',\n",
       "   'Pte',\n",
       "   'ltd',\n",
       "   'in',\n",
       "   'Singapore',\n",
       "   'Monthly',\n",
       "   'preparation',\n",
       "   'of',\n",
       "   'consolidated',\n",
       "   'financial',\n",
       "   'reports',\n",
       "   'budget',\n",
       "   'cost',\n",
       "   'reports',\n",
       "   'for',\n",
       "   'product',\n",
       "   'lines',\n",
       "   'Provide',\n",
       "   'recommendations',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'costs',\n",
       "   'and',\n",
       "   'improve',\n",
       "   'financial',\n",
       "   'performance',\n",
       "   'Ensure',\n",
       "   'all',\n",
       "   'financial',\n",
       "   'practices',\n",
       "   'are',\n",
       "   'in',\n",
       "   'line',\n",
       "   'with',\n",
       "   'al',\n",
       "   'statutory',\n",
       "   'legislation',\n",
       "   'and',\n",
       "   'regulations',\n",
       "   'Confirm',\n",
       "   'assets',\n",
       "   'are',\n",
       "   'being',\n",
       "   'capitalized',\n",
       "   'or',\n",
       "   'expensed',\n",
       "   'in',\n",
       "   'accordance',\n",
       "   'with',\n",
       "   'the',\n",
       "   'company',\n",
       "   'accounting',\n",
       "   'standard',\n",
       "   'and',\n",
       "   'policy',\n",
       "   '11',\n",
       "   '13',\n",
       "   'to',\n",
       "   '11',\n",
       "   '15',\n",
       "   '08',\n",
       "   '13',\n",
       "   'to',\n",
       "   '11',\n",
       "   '13',\n",
       "   '06',\n",
       "   '13',\n",
       "   'to',\n",
       "   '08',\n",
       "   '13',\n",
       "   'Summer',\n",
       "   '2011',\n",
       "   'Accountant',\n",
       "   'at',\n",
       "   'Swiss',\n",
       "   'Fiduciary',\n",
       "   'Advisory',\n",
       "   'Services',\n",
       "   'Pte',\n",
       "   'ltd',\n",
       "   'in',\n",
       "   'Singapore',\n",
       "   'in',\n",
       "   'charge',\n",
       "   'for',\n",
       "   'the',\n",
       "   'accounts',\n",
       "   'of',\n",
       "   '13',\n",
       "   'companies',\n",
       "   'monthly',\n",
       "   'bank',\n",
       "   'reconciliations',\n",
       "   'GST',\n",
       "   'return',\n",
       "   'preparation',\n",
       "   'of',\n",
       "   'Tax',\n",
       "   'Return',\n",
       "   'XBRL',\n",
       "   'Preparation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'financial',\n",
       "   'statements',\n",
       "   'on',\n",
       "   'a',\n",
       "   'quarterly',\n",
       "   'basis',\n",
       "   'introduced',\n",
       "   'business',\n",
       "   'process',\n",
       "   'improvements',\n",
       "   'that',\n",
       "   'enhanced',\n",
       "   'A',\n",
       "   'P',\n",
       "   'functions',\n",
       "   'established',\n",
       "   'common',\n",
       "   'vendor',\n",
       "   'files',\n",
       "   'eliminated',\n",
       "   'duplications',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   'all',\n",
       "   'company',\n",
       "   'secretarial',\n",
       "   'functions',\n",
       "   'duties',\n",
       "   'and',\n",
       "   'responsibilities',\n",
       "   'Project',\n",
       "   'Manager',\n",
       "   'at',\n",
       "   'Intuuchina',\n",
       "   'Shanghai',\n",
       "   'China',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   'cost',\n",
       "   'estimates',\n",
       "   'and',\n",
       "   'planning',\n",
       "   'for',\n",
       "   'foreign',\n",
       "   'companies',\n",
       "   'relocating',\n",
       "   'to',\n",
       "   'China',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   'ensuring',\n",
       "   'best',\n",
       "   'value',\n",
       "   'is',\n",
       "   'obtained',\n",
       "   'from',\n",
       "   'suppliers',\n",
       "   'and',\n",
       "   'balancing',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'internal',\n",
       "   'and',\n",
       "   'external',\n",
       "   'resources',\n",
       "   'consulting',\n",
       "   'Internship',\n",
       "   'at',\n",
       "   'Dongjin',\n",
       "   'consulting',\n",
       "   'Co',\n",
       "   'Ltd',\n",
       "   'Shanghai',\n",
       "   'China',\n",
       "   'Market',\n",
       "   'research',\n",
       "   'on',\n",
       "   'specific',\n",
       "   'topics',\n",
       "   'Generating',\n",
       "   'new',\n",
       "   'business',\n",
       "   'both',\n",
       "   'in',\n",
       "   'face',\n",
       "   'to',\n",
       "   'face',\n",
       "   'meetings',\n",
       "   'and',\n",
       "   'over',\n",
       "   'the',\n",
       "   'phone',\n",
       "   'private',\n",
       "   'Banking',\n",
       "   'Internship',\n",
       "   'at',\n",
       "   'SUNTRUST',\n",
       "   'Investments',\n",
       "   'SA',\n",
       "   'Geneva',\n",
       "   'Switzerland',\n",
       "   'Market',\n",
       "   'analysis',\n",
       "   'upon',\n",
       "   'personal',\n",
       "   'request',\n",
       "   'of',\n",
       "   'private',\n",
       "   'clients',\n",
       "   'Preparing',\n",
       "   'the',\n",
       "   'daily',\n",
       "   'morning',\n",
       "   'news',\n",
       "   'report',\n",
       "   'and',\n",
       "   'performing',\n",
       "   'equity',\n",
       "   'and',\n",
       "   'bond',\n",
       "   'analysis',\n",
       "   'for',\n",
       "   'the',\n",
       "   'trading',\n",
       "   'desk',\n",
       "   'processing',\n",
       "   'validating',\n",
       "   'and',\n",
       "   'reconciliation',\n",
       "   'of',\n",
       "   'client',\n",
       "   'information',\n",
       "   'contracts',\n",
       "   'orders',\n",
       "   'account',\n",
       "   'cash',\n",
       "   'flows',\n",
       "   'languages',\n",
       "   'French',\n",
       "   'English',\n",
       "   'German',\n",
       "   'Mandarin',\n",
       "   'Native',\n",
       "   'language',\n",
       "   'Fluent',\n",
       "   'Intermediate',\n",
       "   'level',\n",
       "   'Beginner',\n",
       "   'Hobbies',\n",
       "   'Skiing',\n",
       "   'Golfing',\n",
       "   'Gym',\n",
       "   'Managing',\n",
       "   'an',\n",
       "   'investment',\n",
       "   'portfolio',\n",
       "   'since',\n",
       "   '2011']}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List to hold the list of tokens of each file which will be stored as the dictionary of value of the key, which is the filename\n",
    "token_list = []\n",
    "\n",
    "#using the RegexpTokenizer to tokenize the file text\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")    \n",
    "for each_resume in file_data_list:\n",
    "    token_dict = {}\n",
    "    for k in each_resume:\n",
    "        token_dict[k] = tokenizer.tokenize(each_resume[k])\n",
    "    token_list.append(token_dict)\n",
    "\n",
    "token_list[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the tokens of the file as a list of strings. This format will exist for each dictionary. Where each dictionary will have the file name as the key and the list of tokens as the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Functions to find the length of the number of tokens for all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function which will come handy while calculating the size of the \n",
    "def token_size(token_list):\n",
    "    all_tokens = []\n",
    "    for each_res in token_list:\n",
    "        for key in each_res:\n",
    "            all_tokens += each_res[key]\n",
    "    return len(all_tokens)\n",
    "\n",
    "#function to determine the length of the vocabulary\n",
    "def get_vocab_size(token_list):\n",
    "    all_tokens = []\n",
    "    for each_res in token_list:\n",
    "        for key in each_res:\n",
    "            all_tokens += each_res[key]\n",
    "    return len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the initial token list is : 145413\n",
      "The length of the vocabulary is : 16916\n",
      "The lexical diversity is : 8.59618113029085\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of the initial token list is : \" + str(token_size(token_list)))\n",
    "print(\"The length of the vocabulary is : \" + str(get_vocab_size(token_list)))\n",
    "print(\"The lexical diversity is : \" + str(token_size(token_list)/get_vocab_size(token_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way to determine the size of the token list. This size doesnt indicate anything about the the vocabulary length. it will just serve as a reference to carry out further steps with a checkpoint that token size is decreasing after each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Removing the Context independent/Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the token list after stop word removal : 108404\n",
      "The length of the vocabulary after stop word removal is : 16573\n",
      "The lexical diversity after stop word removal is : 6.541000422373741\n"
     ]
    }
   ],
   "source": [
    "# reading the contents of the stopwords_en.txt file. the path has to changed according to the directory where the file resides\n",
    "path = \"E:/all study resources/Semester2/Data Wrangling/Assignment 1/assign1/task2/\"\n",
    "fh = open(path+\"stopwords_en/stopwords_en.txt\",\"r\")\n",
    "stop_word = fh.read().split(\"\\n\")\n",
    "fh.close()\n",
    "\n",
    "#iterating over the token_list and filtering the stop words from the token list\n",
    "for each_token_list in token_list:\n",
    "    filtered_tokens = []\n",
    "    for k in each_token_list:\n",
    "        filtered_tokens = [token for token in each_token_list[k] if token not in stop_word]\n",
    "        each_token_list[k] = filtered_tokens\n",
    "        \n",
    "print(\"The length of the token list after stop word removal : \" + str(token_size(token_list)))\n",
    "print(\"The length of the vocabulary after stop word removal is : \" + str(get_vocab_size(token_list)))\n",
    "print(\"The lexical diversity after stop word removal is : \" + str(token_size(token_list)/get_vocab_size(token_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Identification of the first 200 meaningful bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 Initial selection of 250 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Hong', 'Kong'), 390),\n",
       " (('cid', '1'), 146),\n",
       " (('due', 'diligence'), 103),\n",
       " (('financial', 'statements'), 99),\n",
       " (('private', 'equity'), 79),\n",
       " (('Pte', 'Ltd'), 69),\n",
       " (('M', 'A'), 62),\n",
       " (('Microsoft', 'Office'), 61),\n",
       " (('WORK', 'EXPERIENCE'), 61),\n",
       " (('University', 'Hong'), 55),\n",
       " (('Bachelor', 'Business'), 54),\n",
       " (('Business', 'Administration'), 52),\n",
       " (('English', 'Mandarin'), 52),\n",
       " (('Financial', 'Services'), 50),\n",
       " (('Asset', 'Management'), 48),\n",
       " (('Private', 'Equity'), 46),\n",
       " (('asset', 'management'), 45),\n",
       " (('real', 'estate'), 45),\n",
       " (('financial', 'models'), 43),\n",
       " (('Word', 'Excel'), 41),\n",
       " (('internal', 'external'), 40),\n",
       " (('Accounting', 'Finance'), 40),\n",
       " (('Fluent', 'English'), 38),\n",
       " (('fund', 'managers'), 37),\n",
       " (('internal', 'control'), 37),\n",
       " (('Business', 'School'), 37),\n",
       " (('2016', 'Present'), 36),\n",
       " (('cash', 'flow'), 36),\n",
       " (('Senior', 'Associate'), 35),\n",
       " (('Certified', 'Public'), 35),\n",
       " (('P', 'L'), 34),\n",
       " (('2015', 'Present'), 33),\n",
       " (('Risk', 'Management'), 33),\n",
       " (('Fund', 'Services'), 32),\n",
       " (('Junior', 'College'), 32),\n",
       " (('Kuala', 'Lumpur'), 31),\n",
       " (('GPA', '3'), 31),\n",
       " (('Class', 'Honours'), 31),\n",
       " (('senior', 'management'), 31),\n",
       " (('Excel', 'PowerPoint'), 31),\n",
       " (('hedge', 'funds'), 30),\n",
       " (('Fixed', 'Income'), 29),\n",
       " (('Ernst', 'Young'), 29),\n",
       " (('internal', 'controls'), 29),\n",
       " (('financial', 'institutions'), 28),\n",
       " (('Standard', 'Chartered'), 28),\n",
       " (('risk', 'management'), 28),\n",
       " (('Fund', 'Accountant'), 27),\n",
       " (('Mobile', '65'), 27),\n",
       " (('years', 'experience'), 27),\n",
       " (('University', 'Singapore'), 27),\n",
       " (('Proficient', 'Microsoft'), 27),\n",
       " (('4', '4'), 27),\n",
       " (('fund', 'management'), 27),\n",
       " (('MS', 'Office'), 26),\n",
       " (('Secondary', 'School'), 26),\n",
       " (('City', 'University'), 26),\n",
       " (('CFA', 'Level'), 25),\n",
       " (('ad', 'hoc'), 25),\n",
       " (('Corporate', 'Finance'), 25),\n",
       " (('High', 'School'), 25),\n",
       " (('Financial', 'Reporting'), 25),\n",
       " (('financial', 'reports'), 24),\n",
       " (('bank', 'reconciliation'), 24),\n",
       " (('Bachelor', 'Science'), 24),\n",
       " (('PROFESSIONAL', 'EXPERIENCE'), 24),\n",
       " (('financial', 'analysis'), 24),\n",
       " (('Equity', 'Research'), 24),\n",
       " (('listed', 'companies'), 24),\n",
       " (('English', 'Fluent'), 24),\n",
       " (('1', 'cid'), 24),\n",
       " (('Singapore', 'Pte'), 23),\n",
       " (('E', 'R'), 23),\n",
       " (('Assistant', 'Manager'), 23),\n",
       " (('Public', 'Accountants'), 23),\n",
       " (('team', 'members'), 23),\n",
       " (('Word', 'PowerPoint'), 23),\n",
       " (('English', 'Chinese'), 23),\n",
       " (('Bachelor', 'Commerce'), 23),\n",
       " (('Vice', 'President'), 23),\n",
       " (('New', 'York'), 23),\n",
       " (('project', 'management'), 22),\n",
       " (('Asia', 'Pacific'), 22),\n",
       " (('The', 'University'), 22),\n",
       " (('Banking', 'Finance'), 22),\n",
       " (('Nanyang', 'Business'), 22),\n",
       " (('management', 'companies'), 22),\n",
       " (('I', 'N'), 22),\n",
       " (('Co', 'Ltd'), 21),\n",
       " (('hong', 'Kong'), 21),\n",
       " (('Company', 'Limited'), 21),\n",
       " (('Kong', 'Institute'), 21),\n",
       " (('Institute', 'Certified'), 21),\n",
       " (('external', 'auditors'), 21),\n",
       " (('Nanyang', 'Technological'), 21),\n",
       " (('Technological', 'University'), 21),\n",
       " (('hedge', 'fund'), 21),\n",
       " (('financial', 'reporting'), 21),\n",
       " (('Microsoft', 'Excel'), 21),\n",
       " (('business', 'units'), 21),\n",
       " (('regulatory', 'requirements'), 20),\n",
       " (('N', 'T'), 20),\n",
       " (('National', 'University'), 20),\n",
       " (('Second', 'Class'), 20),\n",
       " (('financial', 'services'), 20),\n",
       " (('audit', 'engagements'), 20),\n",
       " (('audit', 'procedures'), 20),\n",
       " (('financial', 'statement'), 20),\n",
       " (('Chartered', 'Bank'), 20),\n",
       " (('Hedge', 'Fund'), 20),\n",
       " (('joint', 'venture'), 20),\n",
       " (('Kong', 'China'), 19),\n",
       " (('communication', 'skills'), 19),\n",
       " (('Date', 'Birth'), 19),\n",
       " (('equity', 'funds'), 19),\n",
       " (('Thomson', 'Reuters'), 19),\n",
       " (('Jul', '2013'), 19),\n",
       " (('Excel', 'Word'), 19),\n",
       " (('US', 'GAAP'), 19),\n",
       " (('Microsoft', 'Word'), 19),\n",
       " (('Work', 'Experience'), 19),\n",
       " (('Greater', 'China'), 19),\n",
       " (('written', 'spoken'), 19),\n",
       " (('management', 'reporting'), 18),\n",
       " (('Investment', 'Management'), 18),\n",
       " (('CFA', 'Institute'), 18),\n",
       " (('P', 'E'), 18),\n",
       " (('Singapore', 'Branch'), 18),\n",
       " (('spoken', 'written'), 18),\n",
       " (('Management', 'University'), 18),\n",
       " (('Fund', 'Management'), 18),\n",
       " (('financial', 'data'), 18),\n",
       " (('Financial', 'Analyst'), 18),\n",
       " (('May', '2015'), 18),\n",
       " (('Office', 'Word'), 18),\n",
       " (('business', 'development'), 18),\n",
       " (('corporate', 'secretarial'), 18),\n",
       " (('financial', 'performance'), 17),\n",
       " (('timely', 'manner'), 17),\n",
       " (('Stock', 'Exchange'), 17),\n",
       " (('GCE', 'A'), 17),\n",
       " (('GCE', 'O'), 17),\n",
       " (('accounting', 'system'), 17),\n",
       " (('Real', 'Estate'), 17),\n",
       " (('LLP', 'Singapore'), 17),\n",
       " (('research', 'reports'), 17),\n",
       " (('balance', 'sheet'), 17),\n",
       " (('annual', 'audit'), 17),\n",
       " (('2013', 'Present'), 17),\n",
       " (('HONG', 'KONG'), 17),\n",
       " (('Chartered', 'Accountant'), 16),\n",
       " (('full', 'set'), 16),\n",
       " (('Sep', '2013'), 16),\n",
       " (('day-to', 'day'), 16),\n",
       " (('Aug', '2014'), 16),\n",
       " (('professional', 'EXPERIENCE'), 16),\n",
       " (('Jun', '2015'), 16),\n",
       " (('Research', 'Analyst'), 16),\n",
       " (('Business', 'Development'), 16),\n",
       " (('corporate', 'governance'), 16),\n",
       " (('3', 'years'), 16),\n",
       " (('timely', 'basis'), 16),\n",
       " (('Middle', 'Office'), 16),\n",
       " (('Expected', 'Salary'), 15),\n",
       " (('J', 'P'), 15),\n",
       " (('corporate', 'actions'), 15),\n",
       " (('1', 'year'), 15),\n",
       " (('written', 'English'), 15),\n",
       " (('E', 'S'), 15),\n",
       " (('Sdn', 'Bhd'), 15),\n",
       " (('Aug', '2011'), 15),\n",
       " (('Level', '1'), 15),\n",
       " (('Net', 'Asset'), 15),\n",
       " (('Financial', 'Management'), 15),\n",
       " (('gmail', 'EDUCATION'), 15),\n",
       " (('United', 'Kingdom'), 15),\n",
       " (('Oct', '2014'), 15),\n",
       " (('Deloitte', 'Touche'), 15),\n",
       " (('Audit', 'Associate'), 15),\n",
       " (('team', 'player'), 15),\n",
       " (('variance', 'analysis'), 15),\n",
       " (('The', 'Chinese'), 15),\n",
       " (('Chinese', 'University'), 15),\n",
       " (('institutional', 'investors'), 15),\n",
       " (('Managing', 'Director'), 15),\n",
       " (('Finance', 'Manager'), 15),\n",
       " (('Investment', 'Banking'), 15),\n",
       " (('N', 'A'), 14),\n",
       " (('Authority', 'Singapore'), 14),\n",
       " (('In', 'charge'), 14),\n",
       " (('Kong', 'Limited'), 14),\n",
       " (('Computer', 'Skills'), 14),\n",
       " (('investment', 'opportunities'), 14),\n",
       " (('University', 'London'), 14),\n",
       " (('First', 'Class'), 14),\n",
       " (('Institute', 'Management'), 14),\n",
       " (('day', 'day'), 14),\n",
       " (('Jan', '2015'), 14),\n",
       " (('Chartered', 'Accountants'), 14),\n",
       " (('Research', 'Associate'), 14),\n",
       " (('4', '0'), 14),\n",
       " (('regulatory', 'reporting'), 14),\n",
       " (('Capital', 'Markets'), 14),\n",
       " (('Bachelor', 'Accountancy'), 14),\n",
       " (('May', '2012'), 14),\n",
       " (('listed', 'company'), 14),\n",
       " (('administrative', 'support'), 14),\n",
       " (('Senior', 'Management'), 14),\n",
       " (('Due', 'Diligence'), 14),\n",
       " (('daily', 'basis'), 14),\n",
       " (('Native', 'English'), 14),\n",
       " (('New', 'Zealand'), 14),\n",
       " (('Cantonese', 'Native'), 14),\n",
       " (('N', 'G'), 14),\n",
       " (('cash', 'flows'), 13),\n",
       " (('wide', 'range'), 13),\n",
       " (('cash', 'management'), 13),\n",
       " (('Fluent', 'Mandarin'), 13),\n",
       " (('Monetary', 'Authority'), 13),\n",
       " (('set', 'accounts'), 13),\n",
       " (('junior', 'staff'), 13),\n",
       " (('valuation', 'reports'), 13),\n",
       " (('Excel', 'VBA'), 13),\n",
       " (('Singapore', 'Management'), 13),\n",
       " (('Jul', '2015'), 13),\n",
       " (('A', 'Level'), 13),\n",
       " (('May', '2014'), 13),\n",
       " (('financial', 'instruments'), 13),\n",
       " (('accounting', 'software'), 13),\n",
       " (('lie', 'nts'), 13),\n",
       " (('Oct', '2012'), 13),\n",
       " (('fund', 'accounting'), 13),\n",
       " (('Holdings', 'Limited'), 13),\n",
       " (('May', '2013'), 13),\n",
       " (('B', 'B'), 13),\n",
       " (('data', 'analysis'), 13),\n",
       " (('journal', 'entries'), 13),\n",
       " (('2014', 'Present'), 13),\n",
       " (('Languages', 'Fluent'), 13),\n",
       " (('Internal', 'Audit'), 13),\n",
       " (('A', 'A'), 13),\n",
       " (('Exchange', 'Program'), 13),\n",
       " (('Middle', 'East'), 13),\n",
       " (('service', 'providers'), 13),\n",
       " (('Jan', '2014'), 13),\n",
       " (('Nov', '2013'), 13),\n",
       " (('University', 'Bachelor'), 13),\n",
       " (('Master', 'Science'), 13),\n",
       " (('CPA', 'Australia'), 13),\n",
       " (('working', 'experience'), 13)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code snippet collects all the tokens of all the documents and looks for bigrams in that . The first 250 bigrams are first\n",
    "#extracted. the FreqDist function is used to extract first 250 most appearing bigrams and then it is converted to a list.\n",
    "ngram_list = []\n",
    "for each_res in token_list:\n",
    "    for key in each_res:\n",
    "        ngram_list += each_res[key]\n",
    "\n",
    "bigrams = ngrams(ngram_list,n=2)\n",
    "bigrams = list(FreqDist(bigrams).most_common(250))\n",
    "bigrams\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select 250 bigrams initially rather than selecting 200 bigrams because the bigrams may have numerical value and it is necessary to remove these bigrams as we are assuming that bigrams which have numerical value are not of much assistance to us. We select the first 250 most commonly appearing bigrams and then later, we get rid of numerical bigrams follwed by the selection of the first 200 bigrams only. This way we would have collected the first 200 meanignful bigrams. We are initially collecting 250 to accomodate more bigrams and to consider more bigrams and thereafter ommiting the numerical values which subsequenlty provides a robust set of bigrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 Filtering the numerical Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of only alphabetic bigrams are  : 221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Hong', 'Kong'), 390),\n",
       " (('due', 'diligence'), 103),\n",
       " (('financial', 'statements'), 99),\n",
       " (('private', 'equity'), 79),\n",
       " (('Pte', 'Ltd'), 69),\n",
       " (('M', 'A'), 62),\n",
       " (('Microsoft', 'Office'), 61),\n",
       " (('WORK', 'EXPERIENCE'), 61),\n",
       " (('University', 'Hong'), 55),\n",
       " (('Bachelor', 'Business'), 54),\n",
       " (('Business', 'Administration'), 52),\n",
       " (('English', 'Mandarin'), 52),\n",
       " (('Financial', 'Services'), 50),\n",
       " (('Asset', 'Management'), 48),\n",
       " (('Private', 'Equity'), 46),\n",
       " (('asset', 'management'), 45),\n",
       " (('real', 'estate'), 45),\n",
       " (('financial', 'models'), 43),\n",
       " (('Word', 'Excel'), 41),\n",
       " (('internal', 'external'), 40),\n",
       " (('Accounting', 'Finance'), 40),\n",
       " (('Fluent', 'English'), 38),\n",
       " (('fund', 'managers'), 37),\n",
       " (('internal', 'control'), 37),\n",
       " (('Business', 'School'), 37),\n",
       " (('cash', 'flow'), 36),\n",
       " (('Senior', 'Associate'), 35),\n",
       " (('Certified', 'Public'), 35),\n",
       " (('P', 'L'), 34),\n",
       " (('Risk', 'Management'), 33),\n",
       " (('Fund', 'Services'), 32),\n",
       " (('Junior', 'College'), 32),\n",
       " (('Kuala', 'Lumpur'), 31),\n",
       " (('Class', 'Honours'), 31),\n",
       " (('senior', 'management'), 31),\n",
       " (('Excel', 'PowerPoint'), 31),\n",
       " (('hedge', 'funds'), 30),\n",
       " (('Fixed', 'Income'), 29),\n",
       " (('Ernst', 'Young'), 29),\n",
       " (('internal', 'controls'), 29),\n",
       " (('financial', 'institutions'), 28),\n",
       " (('Standard', 'Chartered'), 28),\n",
       " (('risk', 'management'), 28),\n",
       " (('Fund', 'Accountant'), 27),\n",
       " (('years', 'experience'), 27),\n",
       " (('University', 'Singapore'), 27),\n",
       " (('Proficient', 'Microsoft'), 27),\n",
       " (('fund', 'management'), 27),\n",
       " (('MS', 'Office'), 26),\n",
       " (('Secondary', 'School'), 26),\n",
       " (('City', 'University'), 26),\n",
       " (('CFA', 'Level'), 25),\n",
       " (('ad', 'hoc'), 25),\n",
       " (('Corporate', 'Finance'), 25),\n",
       " (('High', 'School'), 25),\n",
       " (('Financial', 'Reporting'), 25),\n",
       " (('financial', 'reports'), 24),\n",
       " (('bank', 'reconciliation'), 24),\n",
       " (('Bachelor', 'Science'), 24),\n",
       " (('PROFESSIONAL', 'EXPERIENCE'), 24),\n",
       " (('financial', 'analysis'), 24),\n",
       " (('Equity', 'Research'), 24),\n",
       " (('listed', 'companies'), 24),\n",
       " (('English', 'Fluent'), 24),\n",
       " (('Singapore', 'Pte'), 23),\n",
       " (('E', 'R'), 23),\n",
       " (('Assistant', 'Manager'), 23),\n",
       " (('Public', 'Accountants'), 23),\n",
       " (('team', 'members'), 23),\n",
       " (('Word', 'PowerPoint'), 23),\n",
       " (('English', 'Chinese'), 23),\n",
       " (('Bachelor', 'Commerce'), 23),\n",
       " (('Vice', 'President'), 23),\n",
       " (('New', 'York'), 23),\n",
       " (('project', 'management'), 22),\n",
       " (('Asia', 'Pacific'), 22),\n",
       " (('The', 'University'), 22),\n",
       " (('Banking', 'Finance'), 22),\n",
       " (('Nanyang', 'Business'), 22),\n",
       " (('management', 'companies'), 22),\n",
       " (('I', 'N'), 22),\n",
       " (('Co', 'Ltd'), 21),\n",
       " (('hong', 'Kong'), 21),\n",
       " (('Company', 'Limited'), 21),\n",
       " (('Kong', 'Institute'), 21),\n",
       " (('Institute', 'Certified'), 21),\n",
       " (('external', 'auditors'), 21),\n",
       " (('Nanyang', 'Technological'), 21),\n",
       " (('Technological', 'University'), 21),\n",
       " (('hedge', 'fund'), 21),\n",
       " (('financial', 'reporting'), 21),\n",
       " (('Microsoft', 'Excel'), 21),\n",
       " (('business', 'units'), 21),\n",
       " (('regulatory', 'requirements'), 20),\n",
       " (('N', 'T'), 20),\n",
       " (('National', 'University'), 20),\n",
       " (('Second', 'Class'), 20),\n",
       " (('financial', 'services'), 20),\n",
       " (('audit', 'engagements'), 20),\n",
       " (('audit', 'procedures'), 20),\n",
       " (('financial', 'statement'), 20),\n",
       " (('Chartered', 'Bank'), 20),\n",
       " (('Hedge', 'Fund'), 20),\n",
       " (('joint', 'venture'), 20),\n",
       " (('Kong', 'China'), 19),\n",
       " (('communication', 'skills'), 19),\n",
       " (('Date', 'Birth'), 19),\n",
       " (('equity', 'funds'), 19),\n",
       " (('Thomson', 'Reuters'), 19),\n",
       " (('Excel', 'Word'), 19),\n",
       " (('US', 'GAAP'), 19),\n",
       " (('Microsoft', 'Word'), 19),\n",
       " (('Work', 'Experience'), 19),\n",
       " (('Greater', 'China'), 19),\n",
       " (('written', 'spoken'), 19),\n",
       " (('management', 'reporting'), 18),\n",
       " (('Investment', 'Management'), 18),\n",
       " (('CFA', 'Institute'), 18),\n",
       " (('P', 'E'), 18),\n",
       " (('Singapore', 'Branch'), 18),\n",
       " (('spoken', 'written'), 18),\n",
       " (('Management', 'University'), 18),\n",
       " (('Fund', 'Management'), 18),\n",
       " (('financial', 'data'), 18),\n",
       " (('Financial', 'Analyst'), 18),\n",
       " (('Office', 'Word'), 18),\n",
       " (('business', 'development'), 18),\n",
       " (('corporate', 'secretarial'), 18),\n",
       " (('financial', 'performance'), 17),\n",
       " (('timely', 'manner'), 17),\n",
       " (('Stock', 'Exchange'), 17),\n",
       " (('GCE', 'A'), 17),\n",
       " (('GCE', 'O'), 17),\n",
       " (('accounting', 'system'), 17),\n",
       " (('Real', 'Estate'), 17),\n",
       " (('LLP', 'Singapore'), 17),\n",
       " (('research', 'reports'), 17),\n",
       " (('balance', 'sheet'), 17),\n",
       " (('annual', 'audit'), 17),\n",
       " (('HONG', 'KONG'), 17),\n",
       " (('Chartered', 'Accountant'), 16),\n",
       " (('full', 'set'), 16),\n",
       " (('professional', 'EXPERIENCE'), 16),\n",
       " (('Research', 'Analyst'), 16),\n",
       " (('Business', 'Development'), 16),\n",
       " (('corporate', 'governance'), 16),\n",
       " (('timely', 'basis'), 16),\n",
       " (('Middle', 'Office'), 16),\n",
       " (('Expected', 'Salary'), 15),\n",
       " (('J', 'P'), 15),\n",
       " (('corporate', 'actions'), 15),\n",
       " (('written', 'English'), 15),\n",
       " (('E', 'S'), 15),\n",
       " (('Sdn', 'Bhd'), 15),\n",
       " (('Net', 'Asset'), 15),\n",
       " (('Financial', 'Management'), 15),\n",
       " (('gmail', 'EDUCATION'), 15),\n",
       " (('United', 'Kingdom'), 15),\n",
       " (('Deloitte', 'Touche'), 15),\n",
       " (('Audit', 'Associate'), 15),\n",
       " (('team', 'player'), 15),\n",
       " (('variance', 'analysis'), 15),\n",
       " (('The', 'Chinese'), 15),\n",
       " (('Chinese', 'University'), 15),\n",
       " (('institutional', 'investors'), 15),\n",
       " (('Managing', 'Director'), 15),\n",
       " (('Finance', 'Manager'), 15),\n",
       " (('Investment', 'Banking'), 15),\n",
       " (('N', 'A'), 14),\n",
       " (('Authority', 'Singapore'), 14),\n",
       " (('In', 'charge'), 14),\n",
       " (('Kong', 'Limited'), 14),\n",
       " (('Computer', 'Skills'), 14),\n",
       " (('investment', 'opportunities'), 14),\n",
       " (('University', 'London'), 14),\n",
       " (('First', 'Class'), 14),\n",
       " (('Institute', 'Management'), 14),\n",
       " (('day', 'day'), 14),\n",
       " (('Chartered', 'Accountants'), 14),\n",
       " (('Research', 'Associate'), 14),\n",
       " (('regulatory', 'reporting'), 14),\n",
       " (('Capital', 'Markets'), 14),\n",
       " (('Bachelor', 'Accountancy'), 14),\n",
       " (('listed', 'company'), 14),\n",
       " (('administrative', 'support'), 14),\n",
       " (('Senior', 'Management'), 14),\n",
       " (('Due', 'Diligence'), 14),\n",
       " (('daily', 'basis'), 14),\n",
       " (('Native', 'English'), 14),\n",
       " (('New', 'Zealand'), 14),\n",
       " (('Cantonese', 'Native'), 14),\n",
       " (('N', 'G'), 14),\n",
       " (('cash', 'flows'), 13),\n",
       " (('wide', 'range'), 13),\n",
       " (('cash', 'management'), 13),\n",
       " (('Fluent', 'Mandarin'), 13),\n",
       " (('Monetary', 'Authority'), 13),\n",
       " (('set', 'accounts'), 13),\n",
       " (('junior', 'staff'), 13),\n",
       " (('valuation', 'reports'), 13)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the code snippet looks through the first 250 bigrams and filter out the bigrams with numerical values in them. We extract the \n",
    "# first 200 bigrams from the resultant list\n",
    "\n",
    "#variable to count number of non-numeral bigrams\n",
    "count  = 0\n",
    "\n",
    "#List to hold filtered bigrams\n",
    "filtered_bigrams = []\n",
    "for each_tup in bigrams:\n",
    "    if each_tup[0][0].isalpha() and each_tup[0][1].isalpha():\n",
    "        count += 1\n",
    "        filtered_bigrams.append(each_tup)\n",
    "print(\"The total number of only alphabetic bigrams are  :\" ,count)\n",
    "\n",
    "filtered_bigrams = filtered_bigrams[0:200]\n",
    "filtered_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step has helped us achieve a list of most commonly appearing meaningful bigrams. These would be retokenized into the token list of all the documents so as to make them available as part of the vocabulary. a snapshot of the filtered bigrams is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Consolidation of token by including bigrams in the vocabulary using the MWE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the token list after removing tokens after retokenization with MWE tokenization : 103785\n",
      "The length of the vocabulary after removing tokens after retokenization with MWE tokenization  : 16759\n",
      "The lexical diversity after removing tokens after retokenization with MWE tokenization : 6.192791932692882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'10': ['laurent',\n",
       "   'Lapaire',\n",
       "   'Date',\n",
       "   'birth',\n",
       "   'Nationality',\n",
       "   'Address',\n",
       "   'Mobile',\n",
       "   'Mail',\n",
       "   'Education',\n",
       "   '2017',\n",
       "   '2013',\n",
       "   '2012',\n",
       "   '13',\n",
       "   '2010',\n",
       "   '24',\n",
       "   'April',\n",
       "   '1990',\n",
       "   'Swiss',\n",
       "   '26',\n",
       "   'Jalan',\n",
       "   'Elok',\n",
       "   'Singapore',\n",
       "   '229064',\n",
       "   '65',\n",
       "   '8399',\n",
       "   '9433',\n",
       "   'lapaire',\n",
       "   'laurent',\n",
       "   'gmail',\n",
       "   'Chartered_Accountant',\n",
       "   'Singapore',\n",
       "   'ISCA',\n",
       "   'end',\n",
       "   'bachelor',\n",
       "   'thesis',\n",
       "   'Geneva',\n",
       "   'University',\n",
       "   'bachelor',\n",
       "   'Business_Administration',\n",
       "   'Geneva',\n",
       "   'University',\n",
       "   'faculty',\n",
       "   'SES',\n",
       "   'Social',\n",
       "   'Economic',\n",
       "   'Science',\n",
       "   'Switzerland',\n",
       "   'Thesis',\n",
       "   'future',\n",
       "   'Chinese',\n",
       "   'currency',\n",
       "   'lapaire',\n",
       "   'LJ',\n",
       "   '2013',\n",
       "   'Renminbi',\n",
       "   'dead',\n",
       "   'Two',\n",
       "   'semesters',\n",
       "   'scholarship',\n",
       "   'Yonsei',\n",
       "   'University',\n",
       "   'Seoul',\n",
       "   'South',\n",
       "   'Korea',\n",
       "   'finalize',\n",
       "   'bachelor',\n",
       "   'Business_Administration',\n",
       "   'international',\n",
       "   'exchange',\n",
       "   'program',\n",
       "   'high',\n",
       "   'school',\n",
       "   'degree',\n",
       "   'Collège',\n",
       "   'Calvin',\n",
       "   'Geneva',\n",
       "   'Switzerland',\n",
       "   'specialization',\n",
       "   'law',\n",
       "   'economics',\n",
       "   'Professiona',\n",
       "   'Experience',\n",
       "   '12',\n",
       "   '15',\n",
       "   'present',\n",
       "   'Corporate',\n",
       "   'Services',\n",
       "   'Manager',\n",
       "   'Alpadis',\n",
       "   'Singapore_Pte',\n",
       "   'Singapore',\n",
       "   'Monthly',\n",
       "   'preparation',\n",
       "   'consolidated',\n",
       "   'financial_reports',\n",
       "   'budget',\n",
       "   'cost',\n",
       "   'reports',\n",
       "   'product',\n",
       "   'lines',\n",
       "   'Provide',\n",
       "   'recommendations',\n",
       "   'reduce',\n",
       "   'costs',\n",
       "   'improve',\n",
       "   'financial_performance',\n",
       "   'Ensure',\n",
       "   'financial',\n",
       "   'practices',\n",
       "   'line',\n",
       "   'al',\n",
       "   'statutory',\n",
       "   'legislation',\n",
       "   'regulations',\n",
       "   'Confirm',\n",
       "   'assets',\n",
       "   'capitalized',\n",
       "   'expensed',\n",
       "   'accordance',\n",
       "   'company',\n",
       "   'accounting',\n",
       "   'standard',\n",
       "   'policy',\n",
       "   '11',\n",
       "   '13',\n",
       "   '11',\n",
       "   '15',\n",
       "   '08',\n",
       "   '13',\n",
       "   '11',\n",
       "   '13',\n",
       "   '06',\n",
       "   '13',\n",
       "   '08',\n",
       "   '13',\n",
       "   'Summer',\n",
       "   '2011',\n",
       "   'Accountant',\n",
       "   'Swiss',\n",
       "   'Fiduciary',\n",
       "   'Advisory',\n",
       "   'Services',\n",
       "   'Pte',\n",
       "   'Singapore',\n",
       "   'charge',\n",
       "   'accounts',\n",
       "   '13',\n",
       "   'companies',\n",
       "   'monthly',\n",
       "   'bank',\n",
       "   'reconciliations',\n",
       "   'GST',\n",
       "   'return',\n",
       "   'preparation',\n",
       "   'Tax',\n",
       "   'Return',\n",
       "   'XBRL',\n",
       "   'Preparation',\n",
       "   'financial_statements',\n",
       "   'quarterly',\n",
       "   'basis',\n",
       "   'introduced',\n",
       "   'business',\n",
       "   'process',\n",
       "   'improvements',\n",
       "   'enhanced',\n",
       "   'A',\n",
       "   'P',\n",
       "   'functions',\n",
       "   'established',\n",
       "   'common',\n",
       "   'vendor',\n",
       "   'files',\n",
       "   'eliminated',\n",
       "   'duplications',\n",
       "   'responsible',\n",
       "   'company',\n",
       "   'secretarial',\n",
       "   'functions',\n",
       "   'duties',\n",
       "   'responsibilities',\n",
       "   'Project',\n",
       "   'Manager',\n",
       "   'Intuuchina',\n",
       "   'Shanghai',\n",
       "   'China',\n",
       "   'responsible',\n",
       "   'cost',\n",
       "   'estimates',\n",
       "   'planning',\n",
       "   'foreign',\n",
       "   'companies',\n",
       "   'relocating',\n",
       "   'China',\n",
       "   'responsible',\n",
       "   'ensuring',\n",
       "   'obtained',\n",
       "   'suppliers',\n",
       "   'balancing',\n",
       "   'internal_external',\n",
       "   'resources',\n",
       "   'consulting',\n",
       "   'Internship',\n",
       "   'Dongjin',\n",
       "   'consulting',\n",
       "   'Co_Ltd',\n",
       "   'Shanghai',\n",
       "   'China',\n",
       "   'Market',\n",
       "   'research',\n",
       "   'specific',\n",
       "   'topics',\n",
       "   'Generating',\n",
       "   'business',\n",
       "   'face',\n",
       "   'face',\n",
       "   'meetings',\n",
       "   'phone',\n",
       "   'private',\n",
       "   'Banking',\n",
       "   'Internship',\n",
       "   'SUNTRUST',\n",
       "   'Investments',\n",
       "   'SA',\n",
       "   'Geneva',\n",
       "   'Switzerland',\n",
       "   'Market',\n",
       "   'analysis',\n",
       "   'personal',\n",
       "   'request',\n",
       "   'private',\n",
       "   'clients',\n",
       "   'Preparing',\n",
       "   'daily',\n",
       "   'morning',\n",
       "   'news',\n",
       "   'report',\n",
       "   'performing',\n",
       "   'equity',\n",
       "   'bond',\n",
       "   'analysis',\n",
       "   'trading',\n",
       "   'desk',\n",
       "   'processing',\n",
       "   'validating',\n",
       "   'reconciliation',\n",
       "   'client',\n",
       "   'information',\n",
       "   'contracts',\n",
       "   'orders',\n",
       "   'account',\n",
       "   'cash_flows',\n",
       "   'languages',\n",
       "   'French',\n",
       "   'English',\n",
       "   'German',\n",
       "   'Mandarin',\n",
       "   'Native',\n",
       "   'language',\n",
       "   'Fluent',\n",
       "   'Intermediate',\n",
       "   'level',\n",
       "   'Beginner',\n",
       "   'Hobbies',\n",
       "   'Skiing',\n",
       "   'Golfing',\n",
       "   'Gym',\n",
       "   'Managing',\n",
       "   'investment',\n",
       "   'portfolio',\n",
       "   '2011']}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "#list to hold all the bigrams which will be in the form of tuples. these formalized bigrams will then retokenized using MWE tokenizer \n",
    "# and will be included as part of the vocabulary\n",
    "formalised_bigrams = []\n",
    "for i in range(len(filtered_bigrams)):\n",
    "    formalised_bigrams.append(filtered_bigrams[i][0])\n",
    "\n",
    "#loop that will iterate through the parent token list and update the token list with the top 200 bigrams \n",
    "for each_res in token_list:\n",
    "    for key in each_res:\n",
    "        #passing formalised_bigrams as the parameter for MWETokenizer\n",
    "        mwe_tokenizer = MWETokenizer(formalised_bigrams)\n",
    "        mwe_tokens = mwe_tokenizer.tokenize(each_res[key])\n",
    "        each_res[key] = mwe_tokens\n",
    "\n",
    "print(\"The length of the token list after removing tokens after retokenization with MWE tokenization : \" + str(token_size(token_list)))\n",
    "print(\"The length of the vocabulary after removing tokens after retokenization with MWE tokenization  : \" + str(get_vocab_size(token_list)))\n",
    "print(\"The lexical diversity after removing tokens after retokenization with MWE tokenization : \" + str(token_size(token_list)/get_vocab_size(token_list)))\n",
    "token_list[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is necessary after the removal of stop words and before stemming because, a combination of stop words are most likely to appear consecutively as they are context independent . Therefore, removing the stop words and then finding the bigrams makes sense. Futhermore, it is essential to do this before stemming because once the tokens are stemmed, the tokens get truncated and the bigrams on a whole would not convey any meaning. hence , the identification of bigrams at this stage is justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Stemming of lower case tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the token list after stemming is: 103785\n",
      "The length of the vocabulary after stemming is : 14398\n",
      "The lexical diversity after stemming is: 7.208292818447006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'10': ['laurent',\n",
       "   'Lapaire',\n",
       "   'Date',\n",
       "   'birth',\n",
       "   'Nationality',\n",
       "   'Address',\n",
       "   'Mobile',\n",
       "   'Mail',\n",
       "   'Education',\n",
       "   '2017',\n",
       "   '2013',\n",
       "   '2012',\n",
       "   '13',\n",
       "   '2010',\n",
       "   '24',\n",
       "   'April',\n",
       "   '1990',\n",
       "   'Swiss',\n",
       "   '26',\n",
       "   'Jalan',\n",
       "   'Elok',\n",
       "   'Singapore',\n",
       "   '229064',\n",
       "   '65',\n",
       "   '8399',\n",
       "   '9433',\n",
       "   'lapair',\n",
       "   'laurent',\n",
       "   'gmail',\n",
       "   'Chartered_Accountant',\n",
       "   'Singapore',\n",
       "   'ISCA',\n",
       "   'end',\n",
       "   'bachelor',\n",
       "   'thesi',\n",
       "   'Geneva',\n",
       "   'University',\n",
       "   'bachelor',\n",
       "   'Business_Administration',\n",
       "   'Geneva',\n",
       "   'University',\n",
       "   'faculti',\n",
       "   'SES',\n",
       "   'Social',\n",
       "   'Economic',\n",
       "   'Science',\n",
       "   'Switzerland',\n",
       "   'Thesis',\n",
       "   'futur',\n",
       "   'Chinese',\n",
       "   'currenc',\n",
       "   'lapair',\n",
       "   'LJ',\n",
       "   '2013',\n",
       "   'Renminbi',\n",
       "   'dead',\n",
       "   'Two',\n",
       "   'semest',\n",
       "   'scholarship',\n",
       "   'Yonsei',\n",
       "   'University',\n",
       "   'Seoul',\n",
       "   'South',\n",
       "   'Korea',\n",
       "   'final',\n",
       "   'bachelor',\n",
       "   'Business_Administration',\n",
       "   'intern',\n",
       "   'exchang',\n",
       "   'program',\n",
       "   'high',\n",
       "   'school',\n",
       "   'degre',\n",
       "   'Collège',\n",
       "   'Calvin',\n",
       "   'Geneva',\n",
       "   'Switzerland',\n",
       "   'special',\n",
       "   'law',\n",
       "   'econom',\n",
       "   'Professiona',\n",
       "   'Experience',\n",
       "   '12',\n",
       "   '15',\n",
       "   'present',\n",
       "   'Corporate',\n",
       "   'Services',\n",
       "   'Manager',\n",
       "   'Alpadis',\n",
       "   'Singapore_Pte',\n",
       "   'Singapore',\n",
       "   'Monthly',\n",
       "   'prepar',\n",
       "   'consolid',\n",
       "   'financial_reports',\n",
       "   'budget',\n",
       "   'cost',\n",
       "   'report',\n",
       "   'product',\n",
       "   'line',\n",
       "   'Provide',\n",
       "   'recommend',\n",
       "   'reduc',\n",
       "   'cost',\n",
       "   'improv',\n",
       "   'financial_performance',\n",
       "   'Ensure',\n",
       "   'financi',\n",
       "   'practic',\n",
       "   'line',\n",
       "   'al',\n",
       "   'statutori',\n",
       "   'legisl',\n",
       "   'regul',\n",
       "   'Confirm',\n",
       "   'asset',\n",
       "   'capit',\n",
       "   'expens',\n",
       "   'accord',\n",
       "   'compani',\n",
       "   'account',\n",
       "   'standard',\n",
       "   'polici',\n",
       "   '11',\n",
       "   '13',\n",
       "   '11',\n",
       "   '15',\n",
       "   '08',\n",
       "   '13',\n",
       "   '11',\n",
       "   '13',\n",
       "   '06',\n",
       "   '13',\n",
       "   '08',\n",
       "   '13',\n",
       "   'Summer',\n",
       "   '2011',\n",
       "   'Accountant',\n",
       "   'Swiss',\n",
       "   'Fiduciary',\n",
       "   'Advisory',\n",
       "   'Services',\n",
       "   'Pte',\n",
       "   'Singapore',\n",
       "   'charg',\n",
       "   'account',\n",
       "   '13',\n",
       "   'compani',\n",
       "   'monthli',\n",
       "   'bank',\n",
       "   'reconcili',\n",
       "   'GST',\n",
       "   'return',\n",
       "   'prepar',\n",
       "   'Tax',\n",
       "   'Return',\n",
       "   'XBRL',\n",
       "   'Preparation',\n",
       "   'financial_statements',\n",
       "   'quarterli',\n",
       "   'basi',\n",
       "   'introduc',\n",
       "   'busi',\n",
       "   'process',\n",
       "   'improv',\n",
       "   'enhanc',\n",
       "   'A',\n",
       "   'P',\n",
       "   'function',\n",
       "   'establish',\n",
       "   'common',\n",
       "   'vendor',\n",
       "   'file',\n",
       "   'elimin',\n",
       "   'duplic',\n",
       "   'respons',\n",
       "   'compani',\n",
       "   'secretari',\n",
       "   'function',\n",
       "   'duti',\n",
       "   'respons',\n",
       "   'Project',\n",
       "   'Manager',\n",
       "   'Intuuchina',\n",
       "   'Shanghai',\n",
       "   'China',\n",
       "   'respons',\n",
       "   'cost',\n",
       "   'estim',\n",
       "   'plan',\n",
       "   'foreign',\n",
       "   'compani',\n",
       "   'reloc',\n",
       "   'China',\n",
       "   'respons',\n",
       "   'ensur',\n",
       "   'obtain',\n",
       "   'supplier',\n",
       "   'balanc',\n",
       "   'internal_external',\n",
       "   'resourc',\n",
       "   'consult',\n",
       "   'Internship',\n",
       "   'Dongjin',\n",
       "   'consult',\n",
       "   'Co_Ltd',\n",
       "   'Shanghai',\n",
       "   'China',\n",
       "   'Market',\n",
       "   'research',\n",
       "   'specif',\n",
       "   'topic',\n",
       "   'Generating',\n",
       "   'busi',\n",
       "   'face',\n",
       "   'face',\n",
       "   'meet',\n",
       "   'phone',\n",
       "   'privat',\n",
       "   'Banking',\n",
       "   'Internship',\n",
       "   'SUNTRUST',\n",
       "   'Investments',\n",
       "   'SA',\n",
       "   'Geneva',\n",
       "   'Switzerland',\n",
       "   'Market',\n",
       "   'analysi',\n",
       "   'person',\n",
       "   'request',\n",
       "   'privat',\n",
       "   'client',\n",
       "   'Preparing',\n",
       "   'daili',\n",
       "   'morn',\n",
       "   'news',\n",
       "   'report',\n",
       "   'perform',\n",
       "   'equiti',\n",
       "   'bond',\n",
       "   'analysi',\n",
       "   'trade',\n",
       "   'desk',\n",
       "   'process',\n",
       "   'valid',\n",
       "   'reconcili',\n",
       "   'client',\n",
       "   'inform',\n",
       "   'contract',\n",
       "   'order',\n",
       "   'account',\n",
       "   'cash_flows',\n",
       "   'languag',\n",
       "   'French',\n",
       "   'English',\n",
       "   'German',\n",
       "   'Mandarin',\n",
       "   'Native',\n",
       "   'languag',\n",
       "   'Fluent',\n",
       "   'Intermediate',\n",
       "   'level',\n",
       "   'Beginner',\n",
       "   'Hobbies',\n",
       "   'Skiing',\n",
       "   'Golfing',\n",
       "   'Gym',\n",
       "   'Managing',\n",
       "   'invest',\n",
       "   'portfolio',\n",
       "   '2011']}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#require the PorterStemmer function available in the nltk.stem package\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#iterating through the token_list, tokenize only the lower case characters\n",
    "for each_res in token_list:\n",
    "    for key in each_res:\n",
    "        stemmed_tokens = [stemmer.stem(token) if token.islower() and not re.search(r'[A-Za-z]+_[A-Za-z]+',token) else token for  token in  each_res[key]]\n",
    "        each_res[key] = stemmed_tokens\n",
    "        \n",
    "print(\"The length of the token list after stemming is: \" + str(token_size(token_list)))\n",
    "print(\"The length of the vocabulary after stemming is : \" + str(get_vocab_size(token_list)))\n",
    "print(\"The lexical diversity after stemming is: \" + str(token_size(token_list)/get_vocab_size(token_list)))\n",
    "\n",
    "token_list[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform stemming on the lower case tokens only as the Porterstemmer function by default converts all the tokens to lower case irrespective of the case. Hence in order to preserve the upper case tokens with the assumption that they convey specific meaning, we apply stemming only on the lowercase tokens. We can gradually see the stemmer reducing the number of tokens and the vocabulary size. However, the stemming has increased the lexical diversity which forms the base of analysis from an overall perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Removing tokens with length less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the token list after stemming is: 96114\n",
      "The length of the vocabulary after stemming is : 13693\n",
      "The lexical diversity after stemming is: 7.019206894033448\n"
     ]
    }
   ],
   "source": [
    "#iterates through the token list and filter the tokens with length less than 3\n",
    "for each_res in token_list:\n",
    "    for key in each_res:\n",
    "        proper_len_tokens = [token for token in each_res[key] if len(token) >= 3]\n",
    "        each_res[key] = proper_len_tokens\n",
    "\n",
    "print(\"The length of the token list after stemming is: \" + str(token_size(token_list)))\n",
    "print(\"The length of the vocabulary after stemming is : \" + str(get_vocab_size(token_list)))\n",
    "print(\"The lexical diversity after stemming is: \" + str(token_size(token_list)/get_vocab_size(token_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is necessary to remove tokens with length less than 3 considering the tokens less than the length of 3 dont contribute towards the vocabulary and doesnt help in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Removal of context dependent tokes with threshold greater than 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('client', 173),\n",
       " ('manag', 171),\n",
       " ('includ', 158),\n",
       " ('report', 155),\n",
       " ('team', 154),\n",
       " ('compani', 151),\n",
       " ('work', 151),\n",
       " ('2013', 140),\n",
       " ('account', 139),\n",
       " ('perform', 135),\n",
       " ('2015', 134),\n",
       " ('2011', 133),\n",
       " ('Singapore', 132),\n",
       " ('provid', 131),\n",
       " ('invest', 130),\n",
       " ('process', 130),\n",
       " ('fund', 130),\n",
       " ('financi', 128),\n",
       " ('2012', 127),\n",
       " ('2014', 127),\n",
       " ('busi', 126),\n",
       " ('University', 123),\n",
       " ('prepar', 118),\n",
       " ('review', 117),\n",
       " ('servic', 117),\n",
       " ('Management', 117),\n",
       " ('oper', 114),\n",
       " ('2016', 114),\n",
       " ('gmail', 113),\n",
       " ('project', 113),\n",
       " ('Present', 111),\n",
       " ('market', 110),\n",
       " ('2010', 108),\n",
       " ('Email', 108),\n",
       " ('analysi', 105),\n",
       " ('bank', 104),\n",
       " ('Finance', 103),\n",
       " ('audit', 102),\n",
       " ('Hong_Kong', 100),\n",
       " ('Business', 99),\n",
       " ('industri', 98),\n",
       " ('ensur', 95),\n",
       " ('skill', 95),\n",
       " ('intern', 94),\n",
       " ('present', 94),\n",
       " ('portfolio', 93),\n",
       " ('product', 92),\n",
       " ('develop', 92),\n",
       " ('support', 92),\n",
       " ('system', 92),\n",
       " ('valuat', 91),\n",
       " ('May', 91),\n",
       " ('Mandarin', 90),\n",
       " ('issu', 90),\n",
       " ('2008', 90),\n",
       " ('trade', 89),\n",
       " ('meet', 88),\n",
       " ('Financial', 88),\n",
       " ('2009', 88),\n",
       " ('plan', 87),\n",
       " ('2007', 87),\n",
       " ('assist', 87),\n",
       " ('corpor', 86),\n",
       " ('SKILLS', 86),\n",
       " ('EDUCATION', 86),\n",
       " ('relat', 84),\n",
       " ('Jan', 84),\n",
       " ('International', 82),\n",
       " ('transact', 82),\n",
       " ('monthli', 81),\n",
       " ('execut', 81),\n",
       " ('Senior', 81),\n",
       " ('time', 81),\n",
       " ('experi', 81),\n",
       " ('English', 80),\n",
       " ('document', 80),\n",
       " ('Group', 80),\n",
       " ('Chinese', 79),\n",
       " ('activ', 79),\n",
       " ('Accounting', 79),\n",
       " ('Bloomberg', 77),\n",
       " ('complianc', 77),\n",
       " ('Aug', 77),\n",
       " ('Manager', 75),\n",
       " ('Limited', 74),\n",
       " ('School', 74),\n",
       " ('data', 73),\n",
       " ('offic', 72),\n",
       " ('maintain', 72),\n",
       " ('China', 71),\n",
       " ('capit', 71),\n",
       " ('Bank', 71),\n",
       " ('commun', 71),\n",
       " ('key', 71),\n",
       " ('research', 70),\n",
       " ('requir', 70),\n",
       " ('close', 70),\n",
       " ('inform', 69),\n",
       " ('complet', 69),\n",
       " ('Dec', 69),\n",
       " ('Investment', 69),\n",
       " ('respons', 68),\n",
       " ('daili', 68),\n",
       " ('procedur', 68),\n",
       " ('Jul', 68),\n",
       " ('annual', 68),\n",
       " ('Jun', 68),\n",
       " ('investor', 67),\n",
       " ('sale', 66),\n",
       " ('relationship', 65),\n",
       " ('Associate', 65),\n",
       " ('conduct', 65),\n",
       " ('Assistant', 64),\n",
       " ('year', 64),\n",
       " ('model', 64),\n",
       " ('liais', 64),\n",
       " ('asset', 63),\n",
       " ('level', 63),\n",
       " ('improv', 63),\n",
       " ('equiti', 63),\n",
       " ('control', 63),\n",
       " ('monitor', 63),\n",
       " ('Mobile', 62),\n",
       " ('list', 62),\n",
       " ('administr', 61),\n",
       " ('gener', 61),\n",
       " ('Languages', 61),\n",
       " ('Fund', 61),\n",
       " ('knowledg', 61),\n",
       " ('reconcili', 60),\n",
       " ('request', 60),\n",
       " ('financial_statements', 60),\n",
       " ('Level', 60),\n",
       " ('July', 60),\n",
       " ('global', 60),\n",
       " ('risk', 60),\n",
       " ('base', 60),\n",
       " ('Excel', 60),\n",
       " ('financ', 60),\n",
       " ('Education', 59),\n",
       " ('budget', 59),\n",
       " ('Services', 59),\n",
       " ('implement', 59),\n",
       " ('Sep', 59),\n",
       " ('Global', 59),\n",
       " ('posit', 59),\n",
       " ('2017', 58),\n",
       " ('Oct', 58),\n",
       " ('profession', 58),\n",
       " ('depart', 58),\n",
       " ('month', 58),\n",
       " ('firm', 58),\n",
       " ('Asia', 57),\n",
       " ('member', 57),\n",
       " ('Capital', 57),\n",
       " ('Skills', 57),\n",
       " ('2005', 56),\n",
       " ('Cantonese', 56),\n",
       " ('updat', 56),\n",
       " ('due_diligence', 56),\n",
       " ('deal', 56),\n",
       " ('WORK_EXPERIENCE', 56),\n",
       " ('engag', 56),\n",
       " ('organ', 55),\n",
       " ('sector', 55),\n",
       " ('extern', 55),\n",
       " ('custom', 55),\n",
       " ('group', 55),\n",
       " ('effici', 54),\n",
       " ('train', 54),\n",
       " ('event', 54),\n",
       " ('Nov', 54),\n",
       " ('coordin', 54),\n",
       " ('function', 53),\n",
       " ('Team', 53),\n",
       " ('Language', 53),\n",
       " ('PowerPoint', 53),\n",
       " ('AND', 53),\n",
       " ('identifi', 53),\n",
       " ('return', 52),\n",
       " ('2006', 52),\n",
       " ('initi', 52),\n",
       " ('price', 52),\n",
       " ('Bachelor_Business', 52),\n",
       " ('tax', 52),\n",
       " ('June', 52),\n",
       " ('Feb', 52),\n",
       " ('Audit', 52),\n",
       " ('design', 52),\n",
       " ('The', 51),\n",
       " ('potenti', 51),\n",
       " ('educ', 51),\n",
       " ('Analyst', 51),\n",
       " ('payment', 51),\n",
       " ('Company', 51),\n",
       " ('Assisted', 51),\n",
       " ('strategi', 51),\n",
       " ('recommend', 50),\n",
       " ('Apr', 50),\n",
       " ('record', 50)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we import all the functions from the nltk.probability to find out the tokens with highest number of document frequency\n",
    "from nltk.probability import *        \n",
    "uniq_tokens = []        \n",
    "for each in token_list:\n",
    "    for k in each:\n",
    "        # we take the set of token list of each file. Thereby limiting a single occurence of token per file. This way a count of a\n",
    "        # token gives the number of files the token is present in.\n",
    "        uniq_tokens += list(set(each[k]))\n",
    "        \n",
    "freq = FreqDist(uniq_tokens)\n",
    "freq.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing the code to remove all the words greater than 98% directly. we find out the document frequency of the most appearing word and if the document frequency of that is greater than 98% we go ahead with the logic of removing these words from the list. However, if we dont, we do not remove any of these tokens from the token list. This way we are reducing the process cycles of our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.35779816513761"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_tokens.count('client')/218*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the most occuring token doesnt surpass the threshold 98% documet frequency. Hence, we dont remove any of the other tokens since they are arranged in the decreasing order of their document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Removal of tokens with document Frequency less than 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the token list after removing tokens with less than 2% doc frequency: 77029\n",
      "The length of the vocabulary after removing tokens with less than 2% doc frequency : 2489\n",
      "The lexical diversity after removing tokens with less than 2% doc frequency: 30.947770188830855\n"
     ]
    }
   ],
   "source": [
    "# List to hold all the tokens that appear less than 2% documents. this list will be used later to filter out tokens from the parent list\n",
    "less_freq = []    \n",
    "for each in uniq_tokens:\n",
    "    if uniq_tokens.count(each)/218*100 <= 2:\n",
    "            less_freq.append(each)\n",
    "            \n",
    "# filtering the tokens from the parent list against the less_freq list which contains all the tokens that appear in less than 2% documents\n",
    "for each_res in token_list:\n",
    "    for key in each_res:\n",
    "        less_app_tokens = [token for token in each_res[key] if token not in less_freq]\n",
    "        each_res[key] = less_app_tokens\n",
    "\n",
    "print(\"The length of the token list after removing tokens with less than 2% doc frequency: \" + str(token_size(token_list)))\n",
    "print(\"The length of the vocabulary after removing tokens with less than 2% doc frequency : \" + str(get_vocab_size(token_list)))\n",
    "print(\"The lexical diversity after removing tokens with less than 2% doc frequency: \" + str(token_size(token_list)/get_vocab_size(token_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that after the removal of tokens with less than 2% document frequency we have reduced the size of the tokens considerably and the vocabulary size has considerably come down to 2489 tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Creating sparsevector and vocab dictionary file\n",
    "**CountVector**: converts a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Generate sparse count vectors as 29270863_countVec.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparse vector representation has been written to 29270863_countVec.txt.\n"
     ]
    }
   ],
   "source": [
    "# we collate all the tokens of all the files into a single list and then pass this list to create indexes in order to create sparse vector\n",
    "# representation\n",
    "ngram_list = []\n",
    "for each_res in token_list:\n",
    "    for key in each_res:\n",
    "        ngram_list += each_res[key]\n",
    "\n",
    "# we import CountVectorizer from sklearn.feature_extraction.text to perform indexing on the tokens of the token list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")    \n",
    "\n",
    "#creating a file to be written to the file\n",
    "out_file = open(\"./29270863_countVec.txt\", 'w')\n",
    "vocab = list(set(ngram_list))\n",
    "\n",
    "#create a dictionary to hold the index values of all the tokens\n",
    "vocab_dict = {}\n",
    "i = 0\n",
    "for w in vocab:\n",
    "    vocab_dict[w] = i\n",
    "    i = i + 1\n",
    "\n",
    "#for all the tokens in the token_list we write the sparse vector representation of all the resume documents to a file\n",
    "for each_res in token_list:\n",
    "    for key in each_res:\n",
    "        d_idx = [vocab_dict[w] for w in each_res[key]]\n",
    "        out_file.write(\"resume_(\"+key+\"),\")\n",
    "        for k, v in FreqDist(d_idx).items():            \n",
    "            out_file.write(\"{}:{}, \".format(k,v))\n",
    "        out_file.write('\\n')\n",
    "out_file.close()\n",
    "\n",
    "print(\"The sparse vector representation has been written to 29270863_countVec.txt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating the Vocab Dictionary as  29270863_vocab.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary dictionary has been written to 29270863_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "#dictionary to sort the keys of the parent token list\n",
    "sorted_vocab_dict = {}\n",
    "for key in sorted(vocab_dict.keys()):\n",
    "    sorted_vocab_dict[key] = vocab_dict[key]\n",
    "    \n",
    "#printing the Dictionary to the 29270863_vocab.txt file\n",
    "vocab_str = str(sorted_vocab_dict)\n",
    "out_file = open(\"./29270863_vocab.txt\", 'w')\n",
    "out_file.write(vocab_str)\n",
    "out_file.close()\n",
    "\n",
    "print(\"The vocabulary dictionary has been written to 29270863_vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Final Vocabulary\n",
    "The final vocabulary at the end of pre-processing is the set of tokens remaining. The vocab size generated in this manner should be the same as that generated by the CountVectorizer used to create the sparse count vectors below. Since there are 2489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Audit_Associate', 'honour', 'sensit', 'set', '1000', 'transit', 'Revenue', 'Fund', 'Profile', 'Financial_Analyst', 'suffici', 'email', 'pursu', 'doubl', 'Skill', 'Clients', 'school', 'unusu', 'Outlook', 'ledger', 'identifi', 'collect', 'ADDITIONAL', 'impair', 'Life', 'activ', 'purpos', 'References', 'Identify', 'mortgag', 'Conference', 'speak', 'explain', 'Advanced', 'matter', 'notabl', 'Factset', 'High_School', 'sens', 'form', 'Air', 'master', 'leverag', 'Service', 'expatri', 'ownership', 'LLP_Singapore', 'GPA', 'reconcili', 'charg', 'nation', 'dividend', 'CSR', 'Investment_Management', 'Programme', 'Written', 'appropri', 'intellig', 'Initiated', 'salari', 'websit', 'institutional_investors', 'valuation_reports', 'verifi', 'join', 'digit', 'P_L', 'led', 'traine', 'Honour', 'assur', 'Institute', 'adher', 'Others', 'ventur', 'Employment', 'upgrad', 'India', 'Spring', 'Date', 'rais', 'seek', 'adequaci', 'vers', 'multi-task', 'This', 'Cultural', 'deliv', 'subsidiari', 'social', 'strateg', 'secretari', 'spoken_written', 'day_day', 'progress', 'jurisdict', 'memorandum', 'involv', 'Operations', 'Agreements']\n"
     ]
    }
   ],
   "source": [
    "# Code to showcase the final vocabulary\n",
    "print(vocab[0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Final Checks for Sparse Count Vector and  Pre-processing of text\n",
    "To complete the pre-processing, a check of the tokenisation, stemming, removal of stop words, replacememt of bigrams, and the final creation of sparse count vectors can be checked by examining an individual resume as shown below.\n",
    "\n",
    "The sample patent below shows the following:\n",
    "  1.   extraction of document data   \n",
    "  2.   Removal of special characters, Sentence segmentation and case normalization \n",
    "  3.   Tokenisation, and removal of stop words\n",
    "  4.   Extraction of top 200 bigrams, e.g. watches -> watch\n",
    "  5.   Retokenization of bigrams using MWE tokenization\n",
    "  6.   Applying Porterstemmer on lower case tokens\n",
    "  7.   Removing tokens with length less than 3\n",
    "  8.   most and least frequent words with threshold of greater than 98% document frequency and less than 2% frequecy removed            e.g. 'method', 'includes', 'first', 'second'.\n",
    "  9.   reduction to sparse vector and writing the sparse count vector to a file\n",
    "  10.   check of cross-reference to vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have performed all the necessary steps as part of wrangling the data. The output after each step is shown after every step is performed. The progress of the wrangling process is gauged by three parameters : Total number of tokens, Vocabulary size, and Lexical diversity. We can observe that after each step a substantial decrease in the number of tokens and vocab size and increase in the lexical diversity. This could be a reaffirmation that we are moving in the right direction as far as wrangling is concerned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Summary:\n",
    "##### 1. Section 3, showed extraction of data into a list of dictionaries where each dictionary represents a resume document. It also shows some preprocessing\n",
    "##### 2. Section 4,  showed all the predominant parts of parsing like : tokenization, extracting bigrams, removing stop words, removing most frequent and least frequent tokens in terms of a set threshold of document frequency,removal of tokens with length less than 3.\n",
    "##### 3. Section  5, deomnstrated creation of count sparse vector and vocabulary dictionary and writning each them to respective files.\n",
    "\n",
    "The wrangling cycle illustrated here, shows how a text starting with bellow statistics, ended up reduced to more sparse text while conserving the main text feature. The end output of this text, should serve a good basline line for further topic modelling, feature analysis as well as a basis for an efficient retrival system .\n",
    "\n",
    "\n",
    "######################### Text Statistics Before Wrangling ##################################\n",
    "\n",
    "Total number of words: <b>145416</b><br>\n",
    "Total number of vocabs: <b>16917</b><br>\n",
    "Lexical diversity is :  <b>8.595850328072354 %</b><br>\n",
    "\n",
    "######################### Final Text Statistics After Wranling ##################################\n",
    "\n",
    "Total number of words: <b>77030</b><br>\n",
    "Total number of vocabs: <b>2489</b><br>\n",
    "Lexical diversity is : <b>30.94817195660908 %</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "\n",
    "\n",
    "#### <li>https://docs.python.org/3/library/re.html</li>\n",
    "\n",
    "\n",
    "#### <li>https://www.nltk.org/</li>\n",
    "\n",
    "\n",
    "#### <li>http://www.nltk.org/howto/stem.html</li>\n",
    "\n",
    "\n",
    "#### <li>http://www.nltk.org/_modules/nltk/util.html</li>\n",
    "\n",
    "\n",
    "#### <li>https://www.nltk.org/_modules/nltk/probability.html</li>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
